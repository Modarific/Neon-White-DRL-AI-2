AI Agents Learning Neon White and Similar Games via Reinforcement Learning

Neon White and Reinforcement Learning: Current State
----------------------------------------------------

**Neon White** is a fast-paced single-player FPS/platformer known for its parkour movement and speedrun-centric level design. As of 2025, there is little public documentation of anyone specifically training an RL (reinforcement learning) agent to play _Neon White_. The game doesn’t have an official open environment or API for AI training, which likely contributes to the lack of direct projects. Therefore, to understand how one _might_ tackle Neon White with RL, it’s useful to look at similar cases – both academic research and community projects – where AI agents learn **fast-paced first-person shooters or platformers**. These cases illustrate the approaches, challenges, and successes that would be relevant to a Neon White scenario.

RL in Fast-Paced FPS Games
--------------------------

Researchers have made significant strides in applying deep RL to first-person shooters (FPS), often using _simpler game environments as testbeds_. A landmark example is the work by **Lample & Chaplot (2017)**, who trained an agent to play **Doom** (a classic FPS) using **Deep Q-Networks with recurrent layers (DRQN)**. Their architecture combined raw pixel inputs _and_ high-level game features (like whether an enemy is in view) during training. This multi-modal approach _“dramatically improve[d] the training speed and performance”_ of the agent, which eventually **outperformed both the built-in game AI and human players** in Doom deathmatch trialsarxiv.orgarxiv.org. The takeaway is that incorporating game-state information (e.g. health, ammo, enemy presence) alongside visual data can accelerate learning in partially-observable 3D worlds.

Another influential project was the **Visual Doom AI Competition** (2016–2017), which pushed researchers to design agents for various Doom scenarios. Notably, the 2016 winning bot used a technique called **Direct Future Prediction (DFP)** – instead of directly learning “which action is best,” it learned to predict future game outcomes and choose actions to maximize thosemdpi.com. The 2017 winner combined **Asynchronous Advantage Actor-Critic (A3C)** reinforcement learning with **behavior cloning** (learning from human gameplay data)mdpi.com. These competitions yielded open-source code and set performance benchmarks. For instance, one entry from 2017 used TensorFlow and was able to navigate mazes and shoot enemies in Doom using only the agent’s first-person perspective and game variablesmdpi.com.

Moving to more complex 3D environments, **DeepMind’s “Lab” and Quake III experiments** demonstrated the power of multi-agent training. DeepMind created a **Capture-the-Flag FPS environment** (built on Quake III Arena) and trained agents with a custom **“FTW” (For The Win)** algorithm in a massive self-play setup. After about 450,000 training games (with agents playing _against themselves_ and learning collaboratively), the AI agents reached human-level teamwork and tactics; in fact, after ~200k games they could **consistently defeat strong human players** in this FPS modemdpi.com. The agents used deep RL with recurrent neural networks and were trained via **self-play** (a form of multi-agent reinforcement learning). This work underscored that given enough experience and a proper reward structure (flags captured, opponents tagged, etc.), RL can yield superhuman performance even in complex, **partial-observation FPS tasks**. It also showed the importance of _curriculum learning_ – starting agents on simpler tasks or maps and ramping up difficulty – to gradually build up skillmdpi.commdpi.com.

**Key implementation details** in these FPS projects include the choice of algorithms and state/action design. Modern approaches often use **policy gradient methods** like **PPO (Proximal Policy Optimization)** or actor-critic variants, due to their stability in continuous or high-dimensional action spacesmdpi.commdpi.com. For example, one research team connected **Unity’s ML-Agents** toolkit with a custom FPS game (a team-based “survival shooter” in a random maze) and trained bots with PPOmdpi.com. They experimented with different observation spaces: agents that had a **continuous action space** (moving and turning smoothly) and _no visual input_ actually learned best, whereas agents that also used raw pixel observations struggled with target recognition and behaved erraticallymdpi.com. This suggests that when training an AI for a fast shooter, a simplified state representation (e.g. raycast sensors or abstract position info) can sometimes be more effective than high-dimensional vision – especially if computational resources are limitedmdpi.com.

Another case using **Unity ML-Agents** aimed to create _human-like_ FPS bots. Researchers trained an agent for 1v1 arena combat using **behavior cloning** from human replays, then fine-tuned with RL. In a player study, 61% of people could still tell which character was the bot, noting the AI had awkward navigation at timesmdpi.com. However, the gap was smaller when the bot was trained with advanced techniques. Crucially, a comparison in that study showed that adding a **curriculum** (starting the AI against weak opponents, then gradually increasing difficulty) made training _much faster and more successful_: the curriculum-trained agent achieved higher skill _and_ did so in fewer training steps than an agent trained from scratch on the hardest taskmdpi.com. This aligns with broader findings that _curriculum learning and self-play dramatically improve RL outcomes in games_mdpi.commdpi.com.

**Summary of FPS insights:** Through these projects, the AI research community has learned that _reinforcement learning can indeed produce competent, even human-level, players in FPS games_. Achieving this involves: the right algorithms (PPO, A3C, DQN variants), sometimes augmented by supervised pre-training; carefully designed reward signals (for example, +1 for a frag, -1 for dying, small rewards for navigating towards goals); appropriate action spaces (discrete vs continuous, with action masking if necessary); and huge amounts of training experience or clever training regimes (self-play, curriculum, etc.). Many of these ideas would directly translate to an attempt at Neon White – though Neon White’s emphasis on **speed and precision movement** adds another layer of challenge (more on that in the next section).

RL in Fast-Paced Platformers and Speedrunning
---------------------------------------------

Platforming games – especially those involving speedrunning – test an RL agent’s ability to plan and execute long sequences of precise actions under time pressure. There have been both competitions and individual projects tackling this domain:

*   **OpenAI Gym Retro & the Sonic Challenge (2018):** OpenAI introduced **Gym Retro**, a toolkit that wraps classic console games (like Sega Genesis titles) in a standardized RL interfacemedium.datadriveninvestor.com. Using Gym Retro, they hosted the _Retro Contest_ focused on **Sonic the Hedgehog**, a speedy platformer. Participants had to train agents on a few Sonic levels and then generalize to new, unseen levels – effectively a test of both performance and generalization. The top results were achieved by tweaking state-of-the-art RL algorithms (**PPO and Rainbow DQN** were mentioned)openai.com. In fact, OpenAI reported that _“though many approaches were tried, top results all came from tuning or extending existing algorithms such as PPO and Rainbow”_openai.com. This means no special hard-coded tricks outdid the fundamentals of deep RL. The winning agents were able to complete new levels to a decent extent, but not perfectly – the best score was ~4692 out of 10000 possibleopenai.com. This gap highlighted how challenging **fast platformer games** can be for RL; even advanced algorithms struggled to fully master Sonic’s levels within a reasonable training time. Nonetheless, the contest generated open-source examples (many teams shared their code) and showed that **rewarding progress** (e.g. distance traveled to the right, or level completion) with sparse bonuses for level completion can lead an agent to learn _to run and jump through platform stages_. (In Sonic’s case, time matters indirectly through score and survival rather than an explicit timer reward, but a Neon White agent might use time or completion rank as part of the reward function.)
    
*   **Mario and Other 2D Platformers:** One of the earliest viral examples of game-playing AI was **MarI/O** by SethBling – a project that wasn’t exactly deep RL, but used a genetic algorithm (NEAT) to evolve a neural network to play **Super Mario World**. MarI/O famously learned to finish a level by incremental progress – it died thousands of times but eventually figured out how to avoid pits and go rightmedium.datadriveninvestor.com. In the realm of true reinforcement learning, researchers have since trained deep RL agents on **Super Mario Bros** (using OpenAI Gym’s `super-mario-bros` environment or Retro). These agents use **discrete actions** (run, jump, etc.) and typically DQN or PPO. They can learn basic level completion, though often they require millions of frames of experience or help from imitation learning. An interesting case study is **Celeste**, a notoriously difficult 2D platformer known for precision and speed (much like Neon White but 2D). In 2023, an AI enthusiast documented an attempt to train an agent to _speedrun Celeste_ via reinforcement learning combined with **expert demonstrations** (to help the agent learn complex maneuvers). Over a week of training, the AI gradually improved its time. This and similar community projects (often shared as YouTube videos) show that given the right reward (e.g. negative reward for time taken, positive for level completion) an RL agent _can_ discover strategies to move extremely fast through a platformer. However, pure RL from scratch can be inefficient – hence the use of demonstrations or shaped rewards to guide the learning.
    
*   **Academic research on speedrunning:** Beyond training agents to _execute_ a speedrun, some work has looked at the _planning aspect_. Speedrunning often involves finding an optimal route or sequence of tricks. A 2022 paper by Groß et al. discusses **automating speedrun routing** – essentially modeling a game as a graph and using search or learning to find the fastest patharxiv.org. They survey existing literature and suggest that a combination of graph search, heuristics, and learning could be used to plan out an optimal run. While this is slightly different from end-to-end RL control, it’s relevant for Neon White: an agent might need to not only perform actions quickly but also decide _which shortcut or path_ yields the best time. In principle, RL algorithms with long-horizon planning (or hierarchical RL) could learn these strategies, but this remains a cutting-edge challenge.
    

**Key techniques for platformer RL:** One consistently useful approach is **reward shaping** for speed. In a time-trial game, you might give a higher reward for finishing a level under a shorter time, or even a continuous reward that decreases with time taken. (If applying this to Neon White, the reward could be something like `+100 for level completion minus 1 point per second of time` to encourage the agent to finish _and_ to hurry.) For example, in the **Unity Obstacle Tower** (a 3D platformer/puzzle hybrid environment), the default reward structure gave **+1 for each floor completed and +0.1 for each door unlocked** on the waytwosixtech.com. This incremental reward scheme was crucial – it provided feedback for progress (finding keys, opening doors) so the agent wasn’t waiting until the end of a long level to get any reward. Over time, the agent learned to recognize certain visual cues (like the pixel patterns of doorways) because reaching a door reliably gave rewardtwosixtech.com. A Neon White agent would likely need similar shaping (e.g. small rewards for each demon killed or each checkpoint reached, and a big reward for finishing a level quickly).

Another technique is **imitation learning**. In complex platformers, pure trial-and-error can flounder because the chance of accidentally discovering the optimal strategy (especially if it involves seemingly counter-intuitive moves or timing) is very low. By seeding the agent with human replays, we give it a baseline to improve upon. This was demonstrated in the **Obstacle Tower Challenge**: the winning team recorded themselves playing and then used **behavioral cloning** to pre-train the agent on these trajectoriesblog.aqnichol.com. The agent initially just imitated the human (which, on average, could reach higher floors than a totally random policy). Then they switched to reinforcement learning (PPO) to further train the agent – essentially _fine-tuning_ it. This combination allowed the agent to solve a tricky Sokoban-box-pushing puzzle that pure RL had trouble withblog.aqnichol.com. In fact, the hybrid agent surpassed the human baseline (reaching on average floor 19.4 vs. humans’ 15.6 in testing)blog.aqnichol.com. The **action space** in that 3D platformer was fairly large (54 discrete actions, representing movement directions, jumps, rotations, etc.)twosixtech.com, which is comparable to the complexity a Neon White agent would face (Neon White’s “cards” mechanic essentially adds more action options like special jumps or attacks). The Obstacle Tower agent’s success relied on breaking down the task: first learn to _not fail_ (imitation), then learn to _optimize_ (RL to push further/faster)blog.aqnichol.comblog.aqnichol.com.

**Performance and demos:** We’ve seen demos of RL agents achieving impressive feats in platformers. For example, an **RL-trained agent in the racing game Trackmania** learned to exploit a glitch (a technique called “noseboosting”) to drastically improve lap times, something even human speedrunners find hardyoutube.commanifold.markets. This was achieved by rewarding the agent for faster lap completions, and the agent discovered that crashing in a precise way gave a speed boost – an emergent exploit. This kind of result hints that an agent given the goal to _minimize time_ can come up with creative (even unintended) solutions. In Neon White, an agent might discover shortcuts or trick jumps that save time, potentially even ones unknown to humans, if the environment physics allow it.

To summarize the platformer context: **reinforcement learning can train agents to complete levels and even find speedrun strategies**, but it often requires _massive training iterations_ and benefits greatly from human knowledge (via imitation or reward shaping). Open-source environments like **Gym Retro** make it easy to try this on classic 2D gamesmedium.datadriveninvestor.com, and tools like **Unity ML-Agents** allow creating custom 3D levels that mimic games like Neon White. The successes in Sonic, Mario, Celeste, and Obstacle Tower provide a roadmap for tackling Neon White: use a suitable RL algorithm (PPO is a good starting pointopenai.com), define a reward that heavily incentivizes speed and completion, possibly break the task into curricula (train on easier levels or segments first), and consider leveraging human replays from skilled Neon White players as demonstration data to speed up learning.

Open-Source Resources and Code Repositories
-------------------------------------------

One encouraging aspect for anyone looking to train an AI on Neon White is the wealth of **open-source frameworks and code** from similar projects:

*   **OpenAI’s Gym and Gym Retro:** These provide standardized interfaces for games. For instance, Gym Retro was used in the Sonic contest and is publicly available; it essentially treats a console emulator as an RL environmentmedium.datadriveninvestor.com. While Neon White isn’t a retro game, one could create a _Gym environment_ for it (if one had access to the game’s state). The concept would be to have a loop that feeds the agent’s actions into Neon White and returns the next frame and reward. This is non-trivial without game source code, but mods or computer-vision-based approaches (screen capture and image recognition) have been used by hobbyists to interface RL agents with commercial games.
    
*   **ViZDoom:** The ViZDoom project is an open-source API for Doom that many researchers usemdpi.com. It lets you programmatically control a Doom agent, specifying rewards and getting the raw pixel observations. ViZDoom comes with example scenarios (like “defend the center” or basic maze navigation) and has been the basis for many academic papers. It’s a great example of how a game can be turned into a research-friendly environment. While Neon White’s developers haven’t released an equivalent, someone with game modding skills might create a simplified **Neon White-like environment** in Unity or Unreal and use ML-Agents or a similar interface to train agents.
    
*   **Unity ML-Agents:** Unity’s toolkit is free and well-documentedgithub.comdocs.unity3d.com. It has example projects (for instance, a **Wall-Jump environment** where a character learns to parkour up walls, and a **Crawler** that learns to move a 3D character) which involve platforming skills. Developers can use ML-Agents to set up a custom level – for example, re-create a Neon White level’s geometry and objectives – and then train an agent within Unity. This toolkit handles the messaging between the game and Python RL algorithms. Many GitHub repositories and community forums exist where people share their Unity RL setups (including for FPS scenarios, as noted earlier). Unity’s official examples and the Obstacle Tower environment on GitHubtwosixtech.com provide reference implementations of state spaces, reward functions, and training hyperparameters that one could adapt.
    
*   **Baseline algorithms and libraries:** Reinforcement learning research has yielded a number of libraries like **Stable Baselines (OpenAI Baselines)**, **RLlib**, **Keras-RL**, etc., which implement algorithms like DQN, PPO, A3C in a plug-and-play manner. For instance, OpenAI Baselines’ **PPO implementation** was a starting point for many Sonic contest participants. Competitors often published their training code – e.g., using _Rainbow DQN with data augmentation_ or _PPO with LSTM (recurrent memory)_ – which can be found on GitHub or described in blog posts. The **Pickled ML blog** by one Sonic contestant, for example, explains how standard RL algorithms could get to a certain score and what tricks were needed to push furtherblog.aqnichol.comblog.aqnichol.com.
    
*   **Datasets for imitation:** While there isn’t a dedicated Neon White dataset, the concept would be to collect replays of human speedruns. There are websites (like speedrun.com) and communities who record their runs. These could be repurposed as training data. In research, **learning from demonstrations** is common – for example, Microsoft’s _MineRL_ competition provided a large dataset of Minecraft gameplay for agents to learn mining tasks from human examples. If one were serious about training a Neon White agent, assembling a small dataset of human play (including both successful full runs and failures) could bootstrap the agent’s learning, as was done in Obstacle Towerblog.aqnichol.com.
    

In conclusion, although we didn’t find an off-the-shelf “Neon White RL agent” in the literature, the building blocks are all present in prior work. **Model training approaches** have ranged from pure deep RL (PPO, DQN variants, evolution strategies) to hybrids with imitation and curriculum learning. These approaches required careful **reward design** (especially to handle the speedrunning aspect – encouraging minimum time and level completion) and thoughtful **action/observation space** definitions (sometimes favoring reduced state representations or adding memory for long levels). We’ve seen **implementation details** like using Unity ML-Agents to embed an agent in a game environment, using Gym Retro to leverage emulators, and using frameworks like TensorFlow/PyTorch with RL libraries to actually run the training. **Performance evaluations** across these projects show that agents can reach or exceed human performance in constrained scenarios (Doom battles, Obstacle Tower floors, etc.), and impressive demos exist (AI beating human speedrun times in certain levels, or discovering new exploits). Finally, there is a rich array of **open-source code and environments** – from VizDoom and DeepMind Lab for FPS, to Gym Retro and Unity examples for platformers – that enthusiasts and researchers can draw upon. All these insights and tools would inform any future project where someone tries to train an AI to blaze through Neon White’s heavenly speedrun courses. The challenge is steep, but the community’s experience on similar games provides a proven path to follow.

**Sources:**

*   Lample & Chaplot, _“Playing FPS Games with Deep Reinforcement Learning,”_ arXiv (2017). – Introduced a DRQN-based agent for Doom, combining vision and game features; achieved superhuman performance in Doom deathmatcharxiv.orgarxiv.org.
    
*   **Visual Doom Competition (2016 & 2017)** – Results summarized in an MDPI review: 2016 winner used DFP; 2017 winner used A3C + imitationmdpi.com. Highlights the efficacy of different RL techniques in a fast-paced shooter.
    
*   **MDPI Technologies (2024)** – _“Reinforcement Learning to Train FPS Game Agents”_ (Systematic review and experiments) – Surveys techniques from early SARSA agents to modern deep RL. Notably, finds that self-play/multi-agent training and curriculum learning yield better results than solo trainingmdpi.commdpi.com. Also reports Unity-based FPS experiments where PPO agents without visual input outperformed those with vision (due to state abstraction)mdpi.com, and where curriculum-trained agents outlearned othersmdpi.com.
    
*   Jaderberg et al., _“Human-level performance in Quake III Arena Capture-the-Flag via joint multi-agent reinforcement learning,”_ Science (2019). – DeepMind’s multi-agent FPS study (DeepMind Lab); AI agents trained for 450k games, eventually beating human teamsmdpi.com. Demonstrates scalability of RL in a complex, teamwork-based FPS.
    
*   **OpenAI Retro Contest (2018)** – Official OpenAI summary of resultsopenai.comopenai.com. Shows that standard RL algorithms (PPO, Rainbow) were top performers for Sonic the Hedgehog, and that generalization to new levels was possible but imperfect. Many participant codebases are on GitHub (searchable via OpenAI Retro).
    
*   Nischal Madiraju, “Speed-running games using AI,” _DataDrivenInvestor on Medium_ (2020). – Introductory article explaining how reinforcement learning can be used for game speedruns, referencing tools like OpenAI Gym and the MarI/O examplemedium.datadriveninvestor.commedium.datadriveninvestor.com.
    
*   Groß et al., _“Automating Speedrun Routing: Overview and Vision,”_ arXiv:2106.01182 (2022). – Discusses the challenges of algorithmically finding fastest paths in games (a complement to using RL for execution)arxiv.org. Suggests that deep learning could assist in route optimization for speedrunning.
    
*   **Unity Obstacle Tower Challenge** (2019) – Various sources:
    
        *   _Pickled ML Blog_ by Arthur Juliani (competition winner) – Describes using human demos + PPO to solve more floorsblog.aqnichol.comblog.aqnichol.com. Emphasizes overcoming exploration challenges (e.g. a Sokoban puzzle) via imitation.
            
        *   Two Six Tech Blog, _“Retrospective on Obstacle Tower,”_ by M. Cramer (2019) – Discusses the reward structure (0.1 per door, 1.0 per floor) and technical setuptwosixtech.com. Confirms the difficulty of training (needed ~7.5 million episodes in their setup) and the provided baseline codetwosixtech.comtwosixtech.com.
            
*   **GitHub – SAZZAD-AMT/Training-Agent-for-FPS-game (2021)** – An open-source student project proposing an actor-critic with curriculum learning for Doom. Contains a literature review and references (e.g. to Lample’s work)github.comgithub.com. Illustrates the interest in applying RL to FPS and provides starter code for Doom AI.
    
*   **Reddit r/agi post, “I made a dynamic shooter game NPC with Reinforcement Learning”** (2024) – A forum example of a hobbyist applying Unity ML-Agents to create an FPS enemy NPC. While not a formal source, such posts often detail the observation space (rays, sound sensors) and reward strategy (e.g. +1 for hitting the player) used, aligning with academic approaches. They show that even individual developers are experimenting with RL in FPS, indicating methods are becoming accessible to the public.
